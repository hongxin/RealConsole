# Phase 2: LLM é›†æˆ - è®¾è®¡æ–‡æ¡£

## è®¾è®¡å“²å­¦

**å‚è€ƒ Python æˆåŠŸç»éªŒ + å‘æŒ¥ Rust ä¼˜åŠ¿**

### Python ç‰ˆæœ¬ç²¾å
1. âœ… **ç»Ÿä¸€æ¥å£** - Protocol/ABC æ¸…æ™°
2. âœ… **é‡è¯•æœºåˆ¶** - æŒ‡æ•°é€€é¿ + æŠ–åŠ¨
3. âœ… **ç»Ÿè®¡æ”¶é›†** - å¯è§‚æµ‹æ€§å¼º
4. âœ… **é”™è¯¯å¤„ç†** - åˆ†ç±»æ¸…æ™°
5. âœ… **é™çº§ç­–ç•¥** - Ollama åŒæ¥å£

### Rust ç‹¬ç‰¹ä¼˜åŠ¿
1. ğŸš€ **trait ç³»ç»Ÿ** - æ¯” Protocol æ›´å¼ºå¤§
2. ğŸ›¡ï¸ **ç±»å‹å®‰å…¨** - ç¼–è¯‘æ—¶ä¿è¯
3. âš¡ **async/await** - é«˜æ€§èƒ½å¼‚æ­¥
4. ğŸ”’ **çº¿ç¨‹å®‰å…¨** - Arc + Atomic
5. ğŸ’ **é›¶æˆæœ¬æŠ½è±¡** - æ— è¿è¡Œæ—¶å¼€é”€

---

## æ ¸å¿ƒæ¶æ„

### 1. LlmClient Trait

```rust
use async_trait::async_trait;

#[async_trait]
pub trait LlmClient: Send + Sync {
    /// æ ¸å¿ƒèŠå¤©æ¥å£
    async fn chat(&self, messages: Vec<Message>) -> Result<String, LlmError>;

    /// è·å–æ¨¡å‹åç§°
    fn model(&self) -> &str;

    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    fn stats(&self) -> ClientStats;

    /// è¯Šæ–­è¿æ¥
    async fn diagnose(&self) -> String;
}
```

**ä¼˜åŠ¿**:
- `async_trait` - æ”¯æŒå¼‚æ­¥æ–¹æ³•
- `Send + Sync` - è·¨çº¿ç¨‹å®‰å…¨
- `Result<T, E>` - æ˜¾å¼é”™è¯¯å¤„ç†

### 2. é”™è¯¯å¤„ç†ï¼ˆthiserrorï¼‰

```rust
#[derive(Debug, thiserror::Error)]
pub enum LlmError {
    #[error("Network error: {0}")]
    Network(String),

    #[error("HTTP {status}: {message}")]
    Http { status: u16, message: String },

    #[error("Rate limit exceeded")]
    RateLimit,

    #[error("Timeout after {0:?}")]
    Timeout(Duration),

    #[error("Parse error: {0}")]
    Parse(String),
}
```

**ä¼˜åŠ¿**:
- ç±»å‹å®‰å…¨çš„é”™è¯¯
- è‡ªåŠ¨å®ç° Display + Error
- æ¨¡å¼åŒ¹é…å‹å¥½

### 3. æ¶ˆæ¯ç»“æ„

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Message {
    pub role: MessageRole,
    pub content: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum MessageRole {
    System,
    User,
    Assistant,
}
```

**ä¼˜åŠ¿**:
- ç±»å‹å®‰å…¨
- serde è‡ªåŠ¨åºåˆ—åŒ–
- æšä¸¾çº¦æŸ

### 4. ç»Ÿè®¡ç³»ç»Ÿï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰

```rust
use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::Arc;

#[derive(Debug, Clone)]
pub struct ClientStats {
    total_calls: Arc<AtomicU64>,
    total_retries: Arc<AtomicU64>,
    total_errors: Arc<AtomicU64>,
}

impl ClientStats {
    pub fn record_call(&self) {
        self.total_calls.fetch_add(1, Ordering::Relaxed);
    }

    pub fn total_calls(&self) -> u64 {
        self.total_calls.load(Ordering::Relaxed)
    }
}
```

**ä¼˜åŠ¿**:
- æ— é”å¹¶å‘ï¼ˆAtomicï¼‰
- é›¶æˆæœ¬ï¼ˆArcåªåœ¨cloneæ—¶å¢åŠ å¼•ç”¨è®¡æ•°ï¼‰
- çº¿ç¨‹å®‰å…¨

### 5. é‡è¯•ç­–ç•¥

```rust
pub struct RetryPolicy {
    max_attempts: u32,
    initial_backoff_ms: u64,
    max_backoff_ms: u64,
    backoff_multiplier: f64,
    retryable_status: HashSet<u16>,
}

impl Default for RetryPolicy {
    fn default() -> Self {
        let mut retryable = HashSet::new();
        retryable.extend([429, 500, 502, 503, 504]);

        Self {
            max_attempts: 3,
            initial_backoff_ms: 800,
            max_backoff_ms: 8000,
            backoff_multiplier: 1.8,
            retryable_status: retryable,
        }
    }
}
```

**ä¼˜åŠ¿**:
- å¯é…ç½®
- ç±»å‹å®‰å…¨
- æ¸…æ™°çš„è¯­ä¹‰

---

## å®¢æˆ·ç«¯å®ç°

### Ollama Client

**ç‰¹è‰²åŠŸèƒ½**:
1. åŒæ¥å£é™çº§ (native â†’ OpenAI compatible)
2. æ¨¡å‹åˆ—è¡¨ç¼“å­˜
3. <think> æ ‡ç­¾è¿‡æ»¤
4. ç¦»çº¿æ¨¡å‹å›é€€

```rust
pub struct OllamaClient {
    endpoint: String,
    model: String,
    client: Client,
    stats: ClientStats,
    retry_policy: RetryPolicy,
    model_cache: Arc<Mutex<Option<Vec<String>>>>,
}
```

### Deepseek Client

**ç‰¹è‰²åŠŸèƒ½**:
1. Bearer Token è®¤è¯
2. æ ‡å‡† OpenAI API
3. é€Ÿç‡é™åˆ¶å¤„ç†

```rust
pub struct DeepseekClient {
    endpoint: String,
    model: String,
    api_key: String,
    client: Client,
    stats: ClientStats,
    retry_policy: RetryPolicy,
}
```

---

## é‡è¯•æœºåˆ¶å®ç°

```rust
async fn retry_with_backoff<F, Fut, T>(
    policy: &RetryPolicy,
    mut operation: F,
) -> Result<T, LlmError>
where
    F: FnMut() -> Fut,
    Fut: Future<Output = Result<T, LlmError>>,
{
    let mut backoff_ms = policy.initial_backoff_ms;

    for attempt in 1..=policy.max_attempts {
        match operation().await {
            Ok(result) => return Ok(result),
            Err(e) => {
                if attempt == policy.max_attempts {
                    return Err(e);
                }

                // æ£€æŸ¥æ˜¯å¦å¯é‡è¯•
                if !is_retryable(&e, policy) {
                    return Err(e);
                }

                // æŒ‡æ•°é€€é¿ + æŠ–åŠ¨
                let jitter = rand::random::<u64>() % 100;
                tokio::time::sleep(
                    Duration::from_millis(backoff_ms + jitter)
                ).await;

                backoff_ms = (backoff_ms as f64 * policy.backoff_multiplier) as u64;
                backoff_ms = backoff_ms.min(policy.max_backoff_ms);
            }
        }
    }

    unreachable!()
}
```

---

## é›†æˆåˆ° Agent

### 1. æ‰©å±• Config

```rust
pub struct LlmConfig {
    pub primary: Option<LlmProviderConfig>,
    pub fallback: Option<LlmProviderConfig>,
}

pub struct LlmProviderConfig {
    pub provider: String,  // "ollama", "deepseek", "openai"
    pub model: Option<String>,
    pub endpoint: Option<String>,
    pub api_key: Option<String>,
}
```

### 2. å·¥å‚æ¨¡å¼

```rust
pub fn create_llm_client(
    config: &LlmProviderConfig
) -> Result<Box<dyn LlmClient>, LlmError> {
    match config.provider.as_str() {
        "ollama" => Ok(Box::new(OllamaClient::new(config)?)),
        "deepseek" => Ok(Box::new(DeepseekClient::new(config)?)),
        "openai" => Ok(Box::new(OpenAIClient::new(config)?)),
        _ => Err(LlmError::Parse("Unknown provider".into())),
    }
}
```

### 3. æ–°å¢å‘½ä»¤

```rust
// /ask - LLM å¯¹è¯
fn cmd_ask(arg: &str) -> String {
    // å¼‚æ­¥è°ƒç”¨ LLM
}

// /llm - LLM ç®¡ç†
fn cmd_llm(arg: &str) -> String {
    // status, switch, stats
}
```

---

## æµ‹è¯•ç­–ç•¥

### 1. å•å…ƒæµ‹è¯•

```rust
#[tokio::test]
async fn test_ollama_chat() {
    let client = OllamaClient::new_with_defaults();
    let messages = vec![
        Message::user("Hello"),
    ];
    let result = client.chat(messages).await;
    assert!(result.is_ok());
}
```

### 2. æ¨¡æ‹Ÿæµ‹è¯•ï¼ˆmockitoï¼‰

```rust
#[tokio::test]
async fn test_retry_on_500() {
    let mock = mockito::mock("POST", "/v1/chat/completions")
        .with_status(500)
        .with_body("Internal Error")
        .create();

    // æµ‹è¯•é‡è¯•é€»è¾‘
}
```

### 3. é›†æˆæµ‹è¯•

```rust
#[tokio::test]
#[ignore]  // éœ€è¦çœŸå®æœåŠ¡
async fn test_ollama_integration() {
    // çœŸå® Ollama æµ‹è¯•
}
```

---

## ä¾èµ–æ›´æ–°

```toml
[dependencies]
# å·²æœ‰
tokio = { version = "1.40", features = ["full"] }
reqwest = { version = "0.12", features = ["json"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# æ–°å¢
async-trait = "0.1"
thiserror = "1.0"
rand = "0.8"

[dev-dependencies]
mockito = "1.0"
tokio-test = "0.4"
```

---

## å®æ–½è®¡åˆ’

### Week 1: åŸºç¡€æ¶æ„
- [x] è®¾è®¡æ–‡æ¡£
- [ ] LlmClient trait
- [ ] é”™è¯¯ç±»å‹
- [ ] æ¶ˆæ¯ç»“æ„
- [ ] ç»Ÿè®¡ç³»ç»Ÿ

### Week 2: Ollama
- [ ] OllamaClient å®ç°
- [ ] åŒæ¥å£é™çº§
- [ ] æ¨¡å‹ç®¡ç†
- [ ] å•å…ƒæµ‹è¯•

### Week 3: Deepseek
- [ ] DeepseekClient å®ç°
- [ ] è®¤è¯å¤„ç†
- [ ] é‡è¯•æœºåˆ¶
- [ ] å•å…ƒæµ‹è¯•

### Week 4: é›†æˆ
- [ ] Agent é›†æˆ
- [ ] /ask å‘½ä»¤
- [ ] /llm å‘½ä»¤
- [ ] æ–‡æ¡£æ›´æ–°

---

## æ€§èƒ½ç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡ | Python å¯¹æ¯” |
|------|------|------------|
| é¦–æ¬¡è¯·æ±‚å»¶è¿Ÿ | < 100ms | ~= |
| å¹¶å‘è¯·æ±‚ | 100+ QPS | 10x |
| å†…å­˜å ç”¨ | < 10 MB | 8x |
| CPU å ç”¨ | < 5% | 5x |

---

## é£é™©ä¸ç¼“è§£

### é£é™© 1: å¼‚æ­¥å¤æ‚åº¦
**ç¼“è§£**:
- ä½¿ç”¨ tokio::test
- ç®€åŒ–å¼‚æ­¥è¾¹ç•Œ
- å……åˆ†æµ‹è¯•

### é£é™© 2: é”™è¯¯å¤„ç†
**ç¼“è§£**:
- ä½¿ç”¨ thiserror
- æ¸…æ™°çš„é”™è¯¯ç±»å‹
- å®Œæ•´çš„é”™è¯¯ä¼ æ’­

### é£é™© 3: å…¼å®¹æ€§
**ç¼“è§£**:
- å‚è€ƒ Python å®ç°
- ä¿æŒ API ä¸€è‡´
- å……åˆ†é›†æˆæµ‹è¯•

---

## æˆåŠŸæ ‡å‡†

1. âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡
2. âœ… Clippy æ— è­¦å‘Š
3. âœ… æ€§èƒ½è¾¾æ ‡
4. âœ… æ–‡æ¡£å®Œæ•´
5. âœ… ä¸ Python ç‰ˆæœ¬åŠŸèƒ½å¯¹ç­‰

---

**æ–‡æ¡£ç‰ˆæœ¬**: v0.2.0
**ä½œè€…**: RealConsole Team
**æ—¥æœŸ**: 2025-10-13
